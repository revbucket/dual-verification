{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimum working example for dual zonotopes\n",
    "Whirlwind tour of the codebase with most of the functionality explained (for the naive case)\n",
    "\n",
    "Table of contents:\n",
    "1. Loading a net and setting up an example verification problem \n",
    "2. Computing preactivation bounds / {boxes, zonotopes, polytopes}\n",
    "3. Playing with the naive dual object \n",
    "4. Playing with the decomposed dual object\n",
    "5. Interacting with zonotopes/partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic import block \n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import abstract_domains as ad \n",
    "from neural_nets import FFNet, PreactBounds, KWBounds\n",
    "import train\n",
    "import utilities\n",
    "import dual_naive as dn\n",
    "import dual_decompose as dd \n",
    "import pickle\n",
    "import seaborn as sns \n",
    "sns.set()\n",
    "\n",
    "valsum = lambda d: sum(d.values()) # handy little function\n",
    "flatten = lambda lol: [subel for sublist in lol for subel in sublist]\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Loading a net and setting up an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load and train a net\n",
    "# Simple [784, 256, 128, 64, 10] PGD-trained MNIST network\n",
    "\n",
    "make_net = lambda: FFNet(\n",
    "                    nn.Sequential(nn.Linear(784, 256), \n",
    "                    nn.ReLU(), \n",
    "                    nn.Linear(256, 128), \n",
    "                    nn.ReLU(), \n",
    "                    nn.Linear(128, 64), \n",
    "                    nn.ReLU(), \n",
    "                    nn.Linear(64, 10)))\n",
    "\n",
    "\n",
    "adv_net = make_net() # Make network\n",
    "\n",
    "mnist_train = train.load_mnist_data('train', batch_size=128) # load datasets\n",
    "mnist_val = train.load_mnist_data('val')\n",
    "\n",
    "\n",
    "headless_atk = train.PGD(None, float('inf'), 0.1, 10, lb=0.0, ub=1.0) #setup attack params\n",
    "advtrain_params = train.TrainParameters(mnist_train, mnist_val, 10, adv_attack=headless_atk) # setup train params\n",
    "\n",
    "\n",
    "try: # Try to load the pickled network, otherwise train it\n",
    "    adv_net = pickle.load(open('../pickles/adv_net.pkl', 'rb'))\n",
    "except:\n",
    "    train.training_loop(adv_net, advtrain_params)\n",
    "    pickle.dump(adv_net, open('../pickles/adv_net.pkl', 'wb'))\n",
    "    train.test_validation(adv_net, advtrain_params)\n",
    "\n",
    "advtrain_params.adv_attack = train.PGD(adv_net, float('inf'), 0.1, 10, lb=0.0, ub=1.0)\n",
    "print('Clean acc: %.2f; Robust acc: %.2f' % train.test_validation(adv_net, advtrain_params)[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Now convert network to a binary classifier: (can only do binary classifier certification).\n",
    "We'll do the following jointly:\n",
    "- pick an MNIST example to certify \n",
    "- build the Hyperbox that defines the adversarial input region (what the adv can do)\n",
    "- build a Binary classifier of <label> vs <label + 1>  (e.g., if the example is a 7, this is a 7vs8 classifier)\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "def setup_ex(x, network, rad): # handy function that does the steps above^\n",
    "    # Returns bin_net, input_domain \n",
    "    test_box = ad.Hyperbox.linf_box(x.view(-1), rad) \n",
    "    ypred = network(x.view(1, -1)).max(dim=1)[1].item()\n",
    "    \n",
    "    bin_net = network.binarize(ypred, ((ypred +1) % 10))\n",
    "    return bin_net, test_box\n",
    "\n",
    "\n",
    "RAD = 0.1\n",
    "test_ex = next(iter(mnist_val))[0][20].view(-1) #Just pick an arbitrary example\n",
    "bin_net, test_input = setup_ex(test_ex, adv_net, RAD)\n",
    "#test_input = test_input.clamp(0.0, 1.0)\n",
    "print(bin_net(test_ex.view(1, 28, 28)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Computing Preactivation Bounds\n",
    "Explaining the Abstract Domain framework I've rebuilt for this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" All this stuff is contained in abstract_domains.py for extensions of the base AbstractDomain class.\n",
    "    \n",
    "    Ultimately we want to compute PreactBounds object which has the intermediate bounds stored in a list.\n",
    "    The API is simple, see below for boilerplate methods for computing preactivation bounds\n",
    "\"\"\"\n",
    "\n",
    "def get_hyperbox_prespec(net, test_input):\n",
    "    # Hyperbox bounds (interval bounds)\n",
    "    bounds = PreactBounds(net, test_input, ad.Hyperbox)\n",
    "    bounds.compute() \n",
    "    return bounds \n",
    "\n",
    "def get_zonobox_prespec(net, test_input):\n",
    "    # Computes zonotopes, but then converts to hyperboxes\n",
    "    bounds = PreactBounds(net, test_input, ad.Zonotope)\n",
    "    bounds.compute() \n",
    "    \n",
    "    bounds.abstract_domain = ad.Hyperbox \n",
    "    bounds.bounds = [_.as_hyperbox() for _ in bounds.bounds]\n",
    "    \n",
    "    return bounds \n",
    "\n",
    "def get_zonotope_prespec(net, test_input):\n",
    "    # Computes zonotopes properly\n",
    "    bounds = PreactBounds(net, test_input, ad.Zonotope)\n",
    "    bounds.compute() \n",
    "    return bounds \n",
    "\n",
    "def get_polybox_prespec(net, test_input):\n",
    "    # Computes polytopes [Kolter-Wong thing]\n",
    "    # (bounds.bounds is boxes, but we store the whole polytope too)\n",
    "    bounds = PreactBounds(net, test_input, ad.Polytope)\n",
    "    bounds.compute()\n",
    "    return bounds\n",
    "\n",
    "\n",
    "def get_kw_prespec(net, test_input):\n",
    "    bounds = KW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E.g., what I'm saying about zonotopes vs polytopes:\n",
    "zono_bounds = get_zonotope_prespec(bin_net, test_input)\n",
    "#poly_bounds = get_polybox_prespec(bin_net, test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ZONO BOUNDS: [%.2f, %.2f]\" % (zono_bounds.bounds[-1].lbs.item(), zono_bounds.bounds[-1].ubs.item()))\n",
    "#print(\"POLY BOUNDS: [%.2f, %.2f]\" % (poly_bounds.bounds[-1].lbs.item(), poly_bounds.bounds[-1].ubs.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Actually doing the dual verification\n",
    "Let's use the setup from the previous block where we want to lower bound the optimum of minimize `bin_net(x)` over all `x` in `test_input`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For comparison, let's look at what happens when we use box-based inner minimizations\n",
    "# (but intermediate bounds computed from zonotopes)\n",
    "\n",
    "zonobox_bounds = get_zonobox_prespec(bin_net, test_input)\n",
    "zonobox_dual = dn.NaiveDual(bin_net, test_input, preact_domain=ad.Hyperbox, \n",
    "                            prespec_bounds=zonobox_bounds, choice='naive')\n",
    "\n",
    "optim_obj = optim.Adam(zonobox_dual.parameters(), lr=1e-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zonobox_out = zonobox_dual.dual_ascent(1000, verbose=25, optim_obj=optim_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On the other hand, we can do the same with zonotopes (no hyperbox cast)\n",
    "# - run dual ascent for 1k iterations, and then start computing partition stuff \n",
    "zono_dual = dn.NaiveDual(bin_net, test_input, preact_domain=ad.Zonotope, \n",
    "                         choice='naive')\n",
    "\n",
    "optim_obj = optim.Adam(zono_dual.parameters(), lr=1e-2)\n",
    "zono_out = zono_dual.dual_ascent(1000, verbose=25, optim_obj=optim_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And we can examine the contribution of each subproblem to the total lagrangian \n",
    "zono_dual.lagrange_by_var(zono_dual.argmin())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start partitioning by modifying the choice and partition kwargs attr \n",
    "\n",
    "print(\"Lagrange bounds using naive inner min: \", zono_dual.lagrangian(zono_dual.argmin()))\n",
    "\n",
    "\n",
    "zono_dual.choice = 'partition' \n",
    "zono_dual.partition_kwargs = {'num_partitions': 8, 'partition_style': 'fixed'} \n",
    "# num partition is # of partitions per zonotope \n",
    "# partition_style 'fixed' saves partitions, whereas 'random' re-partitions every time\n",
    "print(\"Lagrange bounds when you partition: \", zono_dual.lagrangian(zono_dual.argmin()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this choice of dual variables lambda_, we can get bounds on the MIP subproblems (no partitioning) ...\n",
    "# (note that further optimization of lambda_ will change these bounds)\n",
    "est_bounds = zono_dual.lagrange_bounds({'TimeLimit': 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X's are solved exactly, Z's are tuples with upper/lower bounds from MIP\n",
    "est_bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Decomposition Objects\n",
    "There's an improved lagrangian formulation using lagrangian splitting, but the same idea holds: you can switch box-based relu programming problems to zonotope-based ones. There's some theory that this provides Lagrangians that are no worse than previous bounds, but the main benefit is that iteration is quicker (but the formulation is slightly more tricky to reason about) \n",
    "\n",
    "This is contained in the `dual_decompose.DecompDual` class, and the API is basically the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zono_decomp = dd.DecompDual(bin_net, test_input,  preact_domain=ad.Zonotope, \n",
    "                            choice='naive', zero_dual=True)\n",
    "\n",
    "# The only extra kwarg here is zero_dual, which initializes the dual variables \n",
    "# from the KW2017 paper, giving a slightly better initial bound \n",
    "\n",
    "zero_dual_bound = zono_decomp.lagrangian(zono_decomp.argmin())\n",
    "zono_decomp = dd.DecompDual(bin_net, test_input,  preact_domain=ad.Zonotope, \n",
    "                            choice='naive', zero_dual=False)\n",
    "init_dual_bound = zono_decomp.lagrangian(zono_decomp.argmin())\n",
    "\n",
    "print(\"Zero dual bound: \", zero_dual_bound)\n",
    "print(\"Init dual bound: \", init_dual_bound)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_obj = optim.Adam(zono_decomp.parameters(), lr=1e-2)\n",
    "zono_out = zono_decomp.dual_ascent(500, verbose=25, optim_obj=optim_obj)\n",
    "\n",
    "optim_obj = optim.Adam(zono_decomp.parameters(), lr=1e-3)\n",
    "zono_out = zono_decomp.dual_ascent(500, verbose=25, optim_obj=optim_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zono_decomp.choice = 'partition' \n",
    "zono_decomp.partition_kwargs = {'partition_style': 'fixed', \n",
    "                                'num_partitions': 16}\n",
    "optim_obj = optim.Adam(zono_decomp.parameters(), lr=1e-2)\n",
    "\n",
    "zono_out = zono_decomp.dual_ascent(25, verbose=1, optim_obj=optim_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zono_decomp.lagrange_bounds({'TimeLimit': 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Interacting with Zonotopes/Partitioning\n",
    "Finally, we can consider the various ways we can partition/merge zonotope partitions.\n",
    "First I'll go over how to modify the partitioning of the dual objects, then how to do this for zonotopes in general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider an existing dual object with some partitions \n",
    "print(zono_dual.choice)\n",
    "print(zono_dual.partition_kwargs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examining the actual partitions: \n",
    "# It's a dict with keys pointing to each layer's zonotope\n",
    "zono_dual.partition_kwargs['partitions'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And each layer is a list of \n",
    "print(type(zono_dual.partition_kwargs['partitions'][1]))\n",
    "\n",
    "# Where each element is a tuple like (idxs_of_original, zonotope)\n",
    "zono_dual.partition_kwargs['partitions'][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The only things you'd probably want to do with a dual object is to \n",
    "# 1. reset the partitions \n",
    "# 2. merge existing partitions together \n",
    "zono_dual.partition_kwargs['partitions'] = None  # resents the partitions \n",
    "\n",
    "\n",
    "zono_dual.argmin() # Will remake the partitions \n",
    "zono_dual.shrink_partitions(4) # now 4 partitions per zonotope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### And to examine the individual zonotopes #####\n",
    "zono_ex = zono_dual.preact_bounds.bounds[1]\n",
    "zono_ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can get the center, generator, element-wise lower and upper bounds like \n",
    "print(zono_ex.center.shape, zono_ex.generator.shape)\n",
    "print(zono_ex.lbs, zono_ex.ubs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To solve a vanilla linear program over the zonotope:\n",
    "zono_ex.solve_lp(torch.ones_like(zono_ex.lbs), get_argmin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To solve a relu program: min_z c1@z + c2@relu(z)... \n",
    "c1 = torch.ones_like(zono_ex.lbs)\n",
    "c2 = -torch.ones_like(zono_ex.lbs)\n",
    "zono_ex.solve_relu_mip(c1, c2, apx_params={'TimeLimit': 10}, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create partitions:\n",
    "parts = zono_ex.make_random_partitions(10)\n",
    "parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and to merge partitions back together \n",
    "half_parts_a = ad.Zonotope.merge_partitions(parts[::2])\n",
    "half_parts_b = ad.Zonotope.merge_partitions(parts[1::2])\n",
    "half_parts_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#... and that's all, I think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
